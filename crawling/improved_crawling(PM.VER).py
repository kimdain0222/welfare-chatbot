import requests
from bs4 import BeautifulSoup
import re
import json
import os
from urllib.parse import urljoin
import time

class WelfareCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def crawl_seoul(self):
        """ì„œìš¸ì‹œ ë³µì§€ ì •ë³´ í¬ë¡¤ë§"""
        print("ğŸ”„ ì„œìš¸ì‹œ ë³µì§€ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        seoul_urls = [
            "https://wis.seoul.go.kr/wfs/ywf/sickMan.do",
            "https://wis.seoul.go.kr/wfs/ywf/selfReliance.do",
            "https://wis.seoul.go.kr/wfs/ywf/saveAccnt.do"
        ]
        
        results = []
        for url in seoul_urls:
            try:
                result = self._crawl_single_page(url, "ì„œìš¸")
                if result:
                    results.append(result)
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            except Exception as e:
                print(f"âŒ ì„œìš¸ í¬ë¡¤ë§ ì—ëŸ¬ ({url}): {e}")
                
        return results
    
    def crawl_incheon(self):
        """ì¸ì²œì‹œ ë³µì§€ ì •ë³´ í¬ë¡¤ë§"""
        print("ğŸ”„ ì¸ì²œì‹œ ë³µì§€ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì¸ì²œì‹œ ì²­ë…„ì •ì±… ì‚¬ì´íŠ¸ì—ì„œ ëª©ë¡ í˜ì´ì§€ í¬ë¡¤ë§
        base_url = "https://youth.incheon.go.kr/youthpolicy/youthPolicyInfoList.do"
        
        results = []
        try:
            # ëª©ë¡ í˜ì´ì§€ì—ì„œ ì •ì±… URLë“¤ ìˆ˜ì§‘
            policy_urls = self._get_incheon_policy_urls()
            
            for url in policy_urls[:20]:  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ
                try:
                    result = self._crawl_single_page(url, "ì¸ì²œ")
                    if result:
                        results.append(result)
                    time.sleep(1)
                except Exception as e:
                    print(f"âŒ ì¸ì²œ í¬ë¡¤ë§ ì—ëŸ¬ ({url}): {e}")
                    
        except Exception as e:
            print(f"âŒ ì¸ì²œ í¬ë¡¤ë§ ì´ˆê¸°í™” ì—ëŸ¬: {e}")
            
        return results
    
    def crawl_gyeonggi(self):
        """ê²½ê¸°ë„ ë³µì§€ ì •ë³´ í¬ë¡¤ë§"""
        print("ğŸ”„ ê²½ê¸°ë„ ë³µì§€ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        base_url = "https://youth.gg.go.kr/gg/intro/youth-policy-job-test.do"
        
        results = []
        try:
            # ê²½ê¸°ë„ ì²­ë…„ì •ì±… ëª©ë¡ì—ì„œ URL ìˆ˜ì§‘
            policy_urls = self._get_gyeonggi_policy_urls()
            
            for url in policy_urls[:20]:  # ìµœëŒ€ 20ê°œë¡œ ì œí•œ
                try:
                    result = self._crawl_single_page(url, "ê²½ê¸°")
                    if result:
                        results.append(result)
                    time.sleep(1)
                except Exception as e:
                    print(f"âŒ ê²½ê¸° í¬ë¡¤ë§ ì—ëŸ¬ ({url}): {e}")
                    
        except Exception as e:
            print(f"âŒ ê²½ê¸° í¬ë¡¤ë§ ì´ˆê¸°í™” ì—ëŸ¬: {e}")
            
        return results
    
    def _get_incheon_policy_urls(self):
        """ì¸ì²œì‹œ ì •ì±… URL ëª©ë¡ ìˆ˜ì§‘"""
        urls = []
        try:
            response = self.session.get("https://youth.incheon.go.kr/youthpolicy/youthPolicyInfoList.do")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì •ì±… ë§í¬ë“¤ ì°¾ê¸°
            links = soup.select('a[href*="youthPolicyInfoDetail.do"]')
            for link in links:
                href = link.get('href')
                if href:
                    full_url = urljoin("https://youth.incheon.go.kr/", href)
                    urls.append(full_url)
                    
        except Exception as e:
            print(f"ì¸ì²œ URL ìˆ˜ì§‘ ì—ëŸ¬: {e}")
            
        return urls
    
    def _get_gyeonggi_policy_urls(self):
        """ê²½ê¸°ë„ ì •ì±… URL ëª©ë¡ ìˆ˜ì§‘"""
        urls = []
        try:
            response = self.session.get("https://youth.gg.go.kr/gg/intro/youth-policy-job-test.do?mode=list")
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì •ì±… ë§í¬ë“¤ ì°¾ê¸°
            links = soup.select('a[href*="mode=view"]')
            for link in links:
                href = link.get('href')
                if href:
                    full_url = urljoin("https://youth.gg.go.kr/", href)
                    urls.append(full_url)
                    
        except Exception as e:
            print(f"ê²½ê¸° URL ìˆ˜ì§‘ ì—ëŸ¬: {e}")
            
        return urls
    
    def _crawl_single_page(self, url, region):
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            result = {
                'url': url,
                'region': region,
                'title': '',
                'age_range': [],
                'application_period': '',
                'conditions': '',
                'benefits': ''
            }
            
            # ì œëª© ì¶”ì¶œ
            title_selectors = [
                '.title-area h2',
                '.b-title-box span',
                'h1',
                '.page-title',
                'title'
            ]
            
            for selector in title_selectors:
                title_tag = soup.select_one(selector)
                if title_tag:
                    result['title'] = title_tag.get_text(strip=True)
                    break
            
            if not result['title']:
                result['title'] = 'ì œëª© ì—†ìŒ'
            
            # ë‚´ìš© ì˜ì—­ ì°¾ê¸°
            content_selectors = [
                '.txt-tp1',
                '#detail_con .line-box',
                '.box-gray',
                '.con-box',
                '.content-area',
                '.detail-content'
            ]
            
            content_text = ""
            for selector in content_selectors:
                content_box = soup.select_one(selector)
                if content_box:
                    content_text = content_box.get_text(separator=" ", strip=True)
                    break
            
            if not content_text:
                return None
            
            # ë‚˜ì´ ë²”ìœ„ ì¶”ì¶œ
            result['age_range'] = self._extract_age_range(content_text)
            
            # ì‹ ì²­ê¸°ê°„ ì¶”ì¶œ
            result['application_period'] = self._extract_application_period(content_text)
            
            # ì¡°ê±´/í˜œíƒ ì¶”ì¶œ
            result['conditions'], result['benefits'] = self._extract_conditions_benefits(soup)
            
            return result
            
        except Exception as e:
            print(f"í˜ì´ì§€ í¬ë¡¤ë§ ì—ëŸ¬ ({url}): {e}")
            return None
    
    def _extract_age_range(self, text):
        """ë‚˜ì´ ë²”ìœ„ ì¶”ì¶œ"""
        # ì •ê·œì‹ íŒ¨í„´ë“¤
        patterns = [
            r'(\d{1,2})\s*ì„¸\s*[~\-]\s*(\d{1,2})\s*ì„¸',  # 20ì„¸~29ì„¸
            r'ë§Œ\s*(\d{1,2})\s*ì„¸\s*ì´í•˜',  # ë§Œ 29ì„¸ ì´í•˜
            r'(\d{1,2})\s*[~\-]\s*(\d{1,2})\s*ì„¸',  # 20~29ì„¸
            r'ë§Œ\s*(\d{1,2})\s*[~\-]\s*(\d{1,2})\s*ì„¸'  # ë§Œ 20~29ì„¸
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                if len(match.groups()) == 2:
                    start, end = int(match.group(1)), int(match.group(2))
                    return list(range(start, end + 1))
                else:
                    max_age = int(match.group(1))
                    return list(range(0, max_age + 1))
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì¶œ
        if 'ì²­ë…„' in text or 'ëŒ€í•™ìƒ' in text:
            return list(range(20, 30))
        
        return []
    
    def _extract_application_period(self, text):
        """ì‹ ì²­ê¸°ê°„ ì¶”ì¶œ"""
        patterns = [
            r'ì‹ ì²­ê¸°ê°„[^\d]*(\d{4}[.\-]\d{2}[.\-]\d{2})\s*[~\-]\s*(\d{4}[.\-]\d{2}[.\-]\d{2})',
            r'ì ‘ìˆ˜ê¸°ê°„[^\d]*(\d{4}[.\-]\d{2}[.\-]\d{2})\s*[~\-]\s*(\d{4}[.\-]\d{2}[.\-]\d{2})',
            r'(\d{4}[.\-]\d{2}[.\-]\d{2})\s*[~\-]\s*(\d{4}[.\-]\d{2}[.\-]\d{2})'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                return f"{match.group(1)}~{match.group(2)}"
        
        return 'ë¯¸ì •'
    
    def _extract_conditions_benefits(self, soup):
        """ì¡°ê±´ê³¼ í˜œíƒ ì¶”ì¶œ"""
        conditions = ""
        benefits = ""
        
        # ì¡°ê±´/í˜œíƒ ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
        sections = soup.find_all(['h3', 'h4', 'h5'])
        
        for section in sections:
            section_text = section.get_text(strip=True)
            next_elements = section.find_next_siblings(['ul', 'p', 'div'])
            
            content = ""
            for elem in next_elements[:3]:  # ìµœëŒ€ 3ê°œ ìš”ì†Œê¹Œì§€ë§Œ
                if elem.name == 'ul':
                    content += elem.get_text(separator=' ', strip=True) + " "
                elif elem.name in ['p', 'div']:
                    content += elem.get_text(strip=True) + " "
            
            if any(keyword in section_text for keyword in ['ì§€ì›ëŒ€ìƒ', 'ì‚¬ì—…ëŒ€ìƒ', 'ì‹ ì²­ìê²©', 'ì§€ì›ìê²©', 'ì¡°ê±´']):
                conditions += content
            elif any(keyword in section_text for keyword in ['ì‚¬ì—…ë‚´ìš©', 'ì§€ì›ë‚´ìš©', 'í˜œíƒ', 'ì§€ì›ê¸ˆì•¡']):
                benefits += content
        
        return conditions.strip(), benefits.strip()
    
    def save_to_json(self, data, filename):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"âœ… {filename}ì— {len(data)}ê°œ ì •ì±… ì €ì¥ ì™„ë£Œ")

def main():
    crawler = WelfareCrawler()
    
    # ê° ì§€ì—­ë³„ í¬ë¡¤ë§
    seoul_data = crawler.crawl_seoul()
    incheon_data = crawler.crawl_incheon()
    gyeonggi_data = crawler.crawl_gyeonggi()
    
    # í†µí•© ë°ì´í„°
    all_data = seoul_data + incheon_data + gyeonggi_data
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    crawler.save_to_json(all_data, 'welfare_policies.json')
    
    print(f"\nğŸ“Š í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"ì„œìš¸: {len(seoul_data)}ê°œ")
    print(f"ì¸ì²œ: {len(incheon_data)}ê°œ")
    print(f"ê²½ê¸°: {len(gyeonggi_data)}ê°œ")
    print(f"ì´ê³„: {len(all_data)}ê°œ")

if __name__ == "__main__":
    main() 